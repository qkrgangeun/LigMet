# lightning.pytorch==2.0.9.post0
seed_everything: 42
trainer:
  accelerator: auto
  strategy: auto
  devices: auto
  num_nodes: 1
  precision: 32-true
  logger: null
  callbacks:
  - class_path: lightning.pytorch.callbacks.ModelCheckpoint
    init_args:
      dirpath: checkpoints
      filename: default-epoch{epoch:02d}-val{val_loss:.4f}
      monitor: val_loss
      verbose: false
      save_last: true
      save_top_k: 1
      save_weights_only: false
      mode: min
      auto_insert_metric_name: true
      every_n_train_steps: null
      train_time_interval: null
      every_n_epochs: null
      save_on_train_epoch_end: null
  fast_dev_run: false
  max_epochs: 100
  min_epochs: null
  max_steps: -1
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: null
  check_val_every_n_epoch: 1
  num_sanity_val_steps: null
  log_every_n_steps: null
  enable_checkpointing: null
  enable_progress_bar: true
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: null
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: null
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: true
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
model:
  model: ligmet.models.Model
  model_config:
    encoder_args:
      input_dim: 224
      hidden_dim: 256
      dropout_rate: 0.1
      output_dim: 32
      egnn_args:
        input_dim: 32
        hidden_dim: 256
        num_layers: 5
        edge_feat_dim: 0
        output_dim: 32
    decoder_args:
      use_attention_prob: false
      use_attention_type: false
      binning_layer_args:
        input_dim: 32
        output_dim: 3
        num_layers: 5
        dropout_rate: 0.1
      prob_transform_args:
        attention:
          input_dim: 32
          embed_dim: 64
          output_dim: 1
          num_heads: 8
          dropout_rate: 0.1
          num_layers: 1
        linear:
          input_dim: 32
          output_dim: 1
          num_layers: 5
          dropout_rate: 0.1
      type_transform_args:
        attention:
          input_dim: 32
          embed_dim: 64
          output_dim: 11
          num_heads: 8
          dropout_rate: 0.1
          num_layers: 1
        linear:
          input_dim: 32
          output_dim: 11
          num_layers: 5
          dropout_rate: 0.1
data:
  dataset_type: onthefly
  train_data_file: /home/qkrgangeun/LigMet/code/text/biolip/train_pdbs.txt
  val_data_file: /home/qkrgangeun/LigMet/code/text/biolip/val_pdbs.txt
  preprocessed:
    features_dir: /home/qkrgangeun/LigMet/data/biolip/DL/features
    rf_result_dir: /home/qkrgangeun/LigMet/data/biolip/rf/results
    topk: 16
    edge_dist_cutoff: 3.0
    pocket_dist: 6
    rf_threshold: 0.5
  onthefly:
    pdb_dir: /home/qkrgangeun/LigMet/data/biolip/pdb
    rf_model: /home/qkrgangeun/LigMet/data/biolip/rf/rf_param/example.param
    topk: 16
    edge_dist_cutoff: 3.0
    pocket_dist: 6
    rf_threshold: 1.0
  train_loader_params:
    batch_size: 1
    num_workers: 1
    shuffle: false
    pin_memory: true
  val_loader_params:
    batch_size: 1
    num_workers: 1
    shuffle: false
    pin_memory: true
optimizer: null
lr_scheduler: null
ckpt_path: null
