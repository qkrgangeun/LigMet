seed_everything: 42
data:
  dataset_type: "onthefly"
  train_data_file: "/home/qkrgangeun/LigMet/code/text/biolip/test_pdbs.txt"
  val_data_file: "/home/qkrgangeun/LigMet/code/text/biolip/val_pdbs.txt"

  preprocessed:
    features_dir: "/home/qkrgangeun/LigMet/data/biolip/DL/features"
    rf_result_dir: "/home/qkrgangeun/LigMet/data/biolip/rf/results"
    topk: 16
    edge_dist_cutoff: 3.0
    pocket_dist: 6
    rf_threshold: 0.5

  onthefly:
    pdb_dir: "/home/qkrgangeun/LigMet/data/biolip/pdb"
    rf_model: "/home/qkrgangeun/LigMet/data/biolip/rf/rf_param/example.param"
    topk: 16
    edge_dist_cutoff: 3.0
    pocket_dist: 6
    rf_threshold: 0.5

  train_loader_params:
    batch_size: 1
    num_workers: 1
    shuffle: True
    pin_memory: True

  val_loader_params:
    batch_size: 1
    num_workers: 1
    shuffle: False
    pin_memory: True

model:
  model: "ligmet.models.Model"
  model_config:
    encoder_args:
      input_dim: 224 
      hidden_dim: 256
      dropout_rate: 0.1
      output_dim: 32
      egnn_args:
        input_dim: 32
        hidden_dim: 256
        num_layers: 5
        edge_feat_dim: 0
        output_dim: 32

    decoder_args:
      use_attention_prob: false
      use_attention_type: false
      binning_layer_args:
        input_dim: 32  
        output_dim: 3  
        num_layers: 5   
        dropout_rate: 0.1   
      prob_transform_args:
        attention:
          input_dim: 32
          embed_dim: 64
          output_dim: 1
          num_heads: 8
          dropout_rate: 0.1
          num_layers: 1
        linear:
          input_dim: 32
          output_dim: 1
          num_layers: 5
          dropout_rate: 0.1

      type_transform_args:
        attention:
          input_dim: 32
          embed_dim: 64
          output_dim: 11
          num_heads: 8
          dropout_rate: 0.1
          num_layers: 1
        linear:
          input_dim: 32
          output_dim: 11
          num_layers: 5
          dropout_rate: 0.1

trainer:
  max_epochs: 100
  accelerator: auto
  devices: auto
  enable_progress_bar: True
  detect_anomaly: True
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "LigMet"
      name: "pocket_metal:none_1:1_default"
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "checkpoints"
        filename: "${model_name}-epoch{epoch}"  
        monitor: "val_loss"
        mode: "min"
        save_top_k: 1
        save_last: True
